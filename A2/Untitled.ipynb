{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('token=hi', 2), ('token=there', 1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, combinations\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import string\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def token_features(tokens, feats):\n",
    "#    for f in tokens:\n",
    "#        feats['token=' + f] = feats['token=' + f] + 1\n",
    "#    keys = abc.keys()\n",
    "#    vals = abc.values()\n",
    "    \n",
    "#    for key in keys:\n",
    "#        newkey.append('token='+key)\n",
    "        \n",
    "    \n",
    "        \n",
    "#    print(sorted(feats.items()))\n",
    "#    feats = dict(c)\n",
    "#    for w in tokens:\n",
    "#        feats['token='+w] = tokens.count(w)\n",
    "    tok = tokens.tolist()\n",
    "    tok = [x.lower() for x in tokens.tolist()]\n",
    "    for x in tok:\n",
    "        feats['token='+x] = tok.count(x)\n",
    "    \n",
    "feats = defaultdict(lambda: 0)    \n",
    "token_features(np.array(['hi', 'there', 'hi']), feats)\n",
    "sorted(feats.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_pair_features(tokens, feats, k=3):\n",
    "#    chunks = [tokens[x:x+k] for x in range(0, len(tokens), k)]\n",
    "    x = 0\n",
    "    chunks = []\n",
    "    combo = []\n",
    "    keys = []\n",
    "    vals = []\n",
    "    final = []\n",
    "    count = Counter()\n",
    "    while (k <= len(tokens)):\n",
    "        chunks.append(tokens[x:k])\n",
    "        x = x + 1\n",
    "        k = k + 1\n",
    "        \n",
    "    for x in chunks:\n",
    "        combo.append(sorted(list(combinations(x,2))))\n",
    "        \n",
    "    for c in (sorted(combo)):\n",
    "        count = count + Counter(c)\n",
    "\n",
    "    d = dict(count.items())\n",
    "    \n",
    "    for v in (sorted(d.items(), key=lambda x: x[0], reverse = False)):\n",
    "        keys.append(v[0])\n",
    "        vals.append(v[1])\n",
    "    \n",
    "    for k in keys:\n",
    "        final.append(k[0]+\"_\"+k[1])\n",
    "    \n",
    "    for x in range(0,len(vals)):\n",
    "        feats[\"token_pair=\"+final[x]] = vals[x]\n",
    "    \n",
    "        \n",
    "\n",
    "feats = defaultdict(lambda: 0)\n",
    "token_pair_features((np.array(['a', 'b', 'c', 'd'])), feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hi', 'there', 'isn', 't', 'this', 'fun'], \n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(doc, punct = False):\n",
    "#    w = doc.split(\"\\'\")\n",
    "#    word = [''.join(x) for x in w]\n",
    "#    for words in w:\n",
    "#        word.append(re.findall(r\"[\\w']+\", words))\n",
    "    word = []\n",
    "    doc = doc.lower()\n",
    "    if punct == True:\n",
    "        word = re.sub('\\W+', ' ', doc.lower()).split()\n",
    "#        word = re.sub(\"[\\W]+\", \" \", doc).split()\n",
    "    if punct == False:\n",
    "#        word = re.findall(r\"[\\w']+\", doc.lower())\n",
    "        word = [word.strip(string.punctuation) for word in doc.split()]\n",
    "    return np.array(word)\n",
    "          \n",
    "tokenize(\" Hi there! Isn't this fun?\", punct=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "word = \"isn't\"\n",
    "w = word.split(\"\\'\")\n",
    "x = [\" \".join(x) for x in w]\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neg_words', 1), ('pos_words', 2)]\n"
     ]
    }
   ],
   "source": [
    "neg_words = set(['bad', 'hate', 'horrible', 'worst', 'boring'])\n",
    "pos_words = set(['awesome', 'amazing', 'best', 'good', 'great', 'love', 'wonderful'])\n",
    "\n",
    "def lexicon_features(tokens, feats):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    feats[\"neg_words\"] = neg\n",
    "    feats[\"pos_words\"] = pos\n",
    "    for words in tokens:\n",
    "        if words.lower() in neg_words:\n",
    "            neg = neg + 1\n",
    "            feats[\"neg_words\"] = neg\n",
    "        if words.lower() in pos_words:\n",
    "            pos = pos + 1\n",
    "            feats[\"pos_words\"] = pos\n",
    "            \n",
    "feats = defaultdict(lambda: 0)\n",
    "lexicon_features(np.array(['i', 'LOVE', 'this', 'great', 'boring', 'movie']), feats)\n",
    "print(sorted(feats.items()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('neg_words', 0),\n",
       " ('pos_words', 2),\n",
       " ('token=great', 1),\n",
       " ('token=i', 1),\n",
       " ('token=love', 1),\n",
       " ('token=movie', 1),\n",
       " ('token=this', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def featurize(tokens, feature_fns):\n",
    "    feats = defaultdict(lambda: 0)\n",
    "    for f in feature_fns:\n",
    "        f(tokens,feats)\n",
    "    return sorted(feats.items())\n",
    "    \n",
    "feats = featurize(np.array(['i', 'LOVE', 'this', 'great', 'movie']), [token_features, lexicon_features])\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x0 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize(tokens_list, feature_fns, min_freq, vocab=None):\n",
    "    \n",
    "    rows = list()\n",
    "    cols = list()\n",
    "    res = list()\n",
    "    flist = []\n",
    "    data = list()\n",
    "    c = Counter()\n",
    "    feats = defaultdict(lambda: 0)\n",
    "    vocabs = defaultdict()\n",
    "    for tokens in tokens_list:\n",
    "        flist.append(featurize(tokens,feature_fns))\n",
    "    for f in sorted(flist):\n",
    "        c[f[0]] += 1\n",
    "    \n",
    "    if vocab == None:\n",
    "        for i,j in enumerate(data):\n",
    "            vocabs[j] = i\n",
    "            \n",
    "    for m,n in enumerate(tokens_list):\n",
    "        feats = featurize(n, feature_fns)\n",
    "        for f in feats:\n",
    "            if f[0] in vocabs:\n",
    "                rows.append(index)\n",
    "                cols.append(vocab[f[0]])\n",
    "                res.append(f[1])\n",
    "                \n",
    "    row = np.array(rows)\n",
    "    column = np.array(cols)\n",
    "    dat = np.array(res)\n",
    "    \n",
    "    return (csr_matrix((dat, (row, column)), shape=(len(tokens_list), len(vocabs)), dtype=np.int64))\n",
    "\n",
    "            \n",
    "docs = [\"Isn't this movie great?\", \"Horrible, horrible movie\"]\n",
    "tokens_list = [tokenize(d) for d in docs]\n",
    "feature_fns = [token_features]\n",
    "vectorize(tokens_list, feature_fns, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 2]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vector(tokens_list, feature_fns, min_freq, vocab=None):\n",
    "    vocabulary = {}\n",
    "    indptr = [0]\n",
    "    data = []\n",
    "    indices = []\n",
    "    for token in tokens_list:\n",
    "        for t in token:\n",
    "            index = vocabulary.setdefault(t, len(vocabulary))\n",
    "            indices.append(index)\n",
    "            data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "    return (csr_matrix((data, indices, indptr), dtype=int).toarray())\n",
    "\n",
    "docs = [\"Isn't this movie great?\", \"Horrible, horrible movie\"]\n",
    "tokens_list = [tokenize(d) for d in docs]\n",
    "feature_fns = [token_features]\n",
    "vector(tokens_list, feature_fns, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no supported conversion for types: (dtype('O'),)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-484-51f1b2c4b841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mtokens_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mfeature_fns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_fns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-483-2c65c5697442>\u001b[0m in \u001b[0;36mvectorize\u001b[0;34m(tokens_list, feature_fns, min_freq, vocab)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mcsr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m#    csr = csr_matrix( (rows,cols), shape=(len(tokens_list),len(feats.keys())) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#    print(type(csr))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     76\u001b[0m                         self.format)\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Read matrix dimensions given, if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36masformat\u001b[0;34m(self, format)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'to'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;31m###################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mtocsr\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             coo_tocsr(M, N, self.nnz, row, col, self.data,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36mupcast\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no supported conversion for types: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: no supported conversion for types: (dtype('O'),)"
     ]
    }
   ],
   "source": [
    "lis=[]\n",
    "    tbool=[]\n",
    "    combs=[]\n",
    "    tbool.append([tokenize(d,keep_internal_punct=True) for d in docs])\n",
    "    tbool.append([tokenize(d,keep_internal_punct=False) for d in docs])\n",
    "    for i in range(1, len(feature_fns)+1):\n",
    "        els = [list(x) for x in combinations(feature_fns, i)]\n",
    "        combs.extend(els)\n",
    "        \n",
    "    for i in range(0,len(tbool)):\n",
    "        for m in min_freqs:\n",
    "            for f in combs:\n",
    "                S,vocab=vectorize(tbool[i], f, m)\n",
    "                accuracy = cross_validation_accuracy(LogisticRegression(), S, labels, 5)\n",
    "                if i==0:\n",
    "                    dic={\"punct\": True,\"features\" : f,\"min_freq\":m,\"accuracy\":accuracy}\n",
    "                else:\n",
    "                    dic={\"punct\": False,\"features\" : f,\"min_freq\":m,\"accuracy\":accuracy}\n",
    "                lis.append(dic)\n",
    "    return sorted(lis, key=lambda k: -k['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_best_classifier(docs, labels, best_result):    \n",
    "    for d in docs:\n",
    "        tokens = tokenize(d,best_result[\"punct\"])\n",
    "        X,vocab=vectorize(tokens,best_result[\"features\"] , nest_result[\"min_freq\"])\n",
    "        best_classifier = LogisticRegression()\n",
    "    return (best_classifier.fit(X,labels),vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_score(truth, predicted):\n",
    "    return len(np.where(truth==predicted)[0]) / len(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation_accuracy(clf, X, labels, k):\n",
    "    kfold = KFold(n_splits=k)\n",
    "    act_acc = list()\n",
    "    for train, test in kfold.split(X):\n",
    "        pred_acc = clf.predict(X[test])\n",
    "        clf.fit(X[train], labels[train])\n",
    "        calc_acc = accuracy_score(labels[test], pred_acc)\n",
    "        act_acc += acc\n",
    "    return (np.mean(act_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_all_combinations(docs, labels, punct_vals,\n",
    "                          feature_fns, min_freqs):\n",
    "    clf = LogisticRegression()\n",
    "    i = 0\n",
    "    result = dict()\n",
    "    eval_list = list()\n",
    "    features = []\n",
    "    for i in range(1, len(feature_fns) + 1):\n",
    "        features.append(combinations(feature_fns, i))\n",
    "    for feature in features:\n",
    "        for punct in punct_vals:\n",
    "            for min_f in min_freqs:\n",
    "                tokens = [tokenize(doc, keep_internal_punct=punct) for doc in docs]\n",
    "                X, vocab = vectorize(tokens, feature, min_freq=min_f)\n",
    "                act_acc = cross_validation_accuracy(clf, X, labels, 5)\n",
    "                result['punct'] = punct\n",
    "                result['features'] = feature\n",
    "                result['min_freq'] = min_f\n",
    "                result['accuracy'] = act_acc\n",
    "                eval_list = eval_list + result\n",
    "                \n",
    "    results = sorted(eval_list, key=lambda x: x['accuracy'], reverse = False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_sorted_accuracies(results):\n",
    "    acc = []\n",
    "    for r in results:\n",
    "        acc.append(r['accuracy'])\n",
    "        \n",
    "    plt.plot(acc)\n",
    "    plt.ylabel('ACC')\n",
    "    plt.show()\n",
    "    plt.savefig(\"accuracies.png\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_accuracy_per_setting(results):\n",
    "    min_freq = list()\n",
    "    punct = list()\n",
    "    feats = list()\n",
    "    values = list()\n",
    "    acc = list()\n",
    "    mean_acc = list()\n",
    "    for result in results:\n",
    "        min_freq.append((\"min_freq\", result['min_freq']))\n",
    "        punct.append((\"punct\", result['punct']))\n",
    "        feats.append((\"features\", result['features']))\n",
    "    values.append(min_freq)\n",
    "    values.append(punct)\n",
    "    values.append(feats)\n",
    "    for val in values:\n",
    "        for result in results:\n",
    "            if result[values[0]]==values[1]:\n",
    "                acc.append(result['accuracy'])\n",
    "                \n",
    "        for freq in values[1]:\n",
    "            setting = \"=\" + \" \".join(i.__name__)\n",
    "        if values[0] == \"features\":\n",
    "            setting = values[0] + setting \n",
    "        else:\n",
    "            setting = values[0] + \"=\" + str(values[1])\n",
    "    \n",
    "    mean = sum(acc)/len(acc)\n",
    "    mean_acc.append((mean,setting))\n",
    "    return sorted(mean_acc, key=lambda x:x[0], reverse = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_coefs(clf, label, n, vocab):\n",
    "    coef = clf.coef_[0]\n",
    "    poslist = list()\n",
    "    neglist = list()\n",
    "    vocabs = sorted(vocab.keys())\n",
    "    vocab_coef = zip(sorted_vocab, coeff)\n",
    "    if label == 1:\n",
    "        for z in vocab_coef:\n",
    "            if z[1] >= 0:\n",
    "                poslist.append(z)\n",
    "        result = sorted(poslist, key=lambda x: x[1], reverse = True)\n",
    "        return result[0:n]\n",
    "    if label == 0:\n",
    "        for z in vocab_coef:\n",
    "            if z[1] < 0:\n",
    "                neglist.append(z)\n",
    "        result = sorted(neglist, key=lambda x: x[1], reverse = False)\n",
    "        return result[0:n]\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_test_data(best_result, vocab):\n",
    "    docs, labels = read_data(os.path.join('data', 'test'))\n",
    "    for doc in docs:\n",
    "        tokens = [tokenize(doc,best_result[\"punct\"])]\n",
    "        \n",
    "    cfr,vocabulary = vectorize(tokens, best_result[\"features\"], best_result[\"min_freq\"], vocab)\n",
    "    return(docs,labels,cfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_misclassified(test_docs, test_labels, X_test, clf, n):\n",
    "    predict = clf.predict(X_test)\n",
    "    probability = clf.predict_proba(X_test)\n",
    "    actual_prob = difaultdict()\n",
    "    result = list()\n",
    "    for x,y in enumerate(predict):\n",
    "        if y == test_labels[x]:\n",
    "            continue\n",
    "        else:\n",
    "            actual_prob[x] = probability[x][y]\n",
    "    temp = sorted(actual_prob.items(), key=lambda x: x[1], reverse = True)\n",
    "    for x in range(0,n):\n",
    "        result.append(temp)\n",
    "    for var in result:\n",
    "        print(\"Actual Truth = \" , test_labels[i[0]], \" \", \"Predicted Truth = \", predict_value[i[0]], \" \",\n",
    "              \"Probability = \", str(i[1]))\n",
    "        print(test_docs[i[0]])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
